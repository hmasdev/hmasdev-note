{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hmasdev's notes","text":"<p>Welcome to hmasdev's notes \ud83d\udc4b</p> <p> </p> enja <p>In this blog, I will share practical knowledge and thoughts about</p> <ul> <li>data science \ud83d\udcca and AI \ud83e\udd16;</li> <li>programming like python ;</li> <li>and creating keyboards \u2328\ufe0f.</li> </ul> <p>I hope to learn and grow together with you through this blog.</p> <p>\u3053\u306e\u30d6\u30ed\u30b0\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u30c6\u30fc\u30de\u306b\u3064\u3044\u3066\u306e\u5b9f\u8df5\u7684\u306a\u77e5\u8b58\u3084\u8003\u3048\u3092\u5171\u6709\u3057\u3066\u3044\u304d\u307e\u3059\uff1a</p> <ul> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9 \ud83d\udcca \u3068AI \ud83e\udd16;</li> <li>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\uff08Python  \u306a\u3069\uff09;</li> <li>\u81ea\u4f5c\u30ad\u30fc\u30dc\u30fc\u30c9 \u2328\ufe0f\u3002</li> </ul> <p>\u3053\u306e\u30d6\u30ed\u30b0\u3092\u901a\u3058\u3066\u3001\u4e00\u7dd2\u306b\u5b66\u3073\u3001\u6210\u9577\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01</p>"},{"location":"#recent-posts","title":"Recent Posts","text":"<p>\ud83d\udea7 TBD \ud83d\udea7</p>"},{"location":"#about-me","title":"About Me","text":"enja <p>I'm working as a data scientist and a machine learning engineer in Tokyo, Japan.</p> <p>I am passionate about solving problems and exploring new ideas with data science and AI, etc.</p> <p>More about me here.</p> <p>\u6771\u4eac\u5728\u4f4f\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u30fb\u6a5f\u68b0\u5b66\u7fd2\u30a8\u30f3\u30b8\u30cb\u30a2\u3002</p> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3084AI\u306a\u3069\u3092\u4f7f\u3063\u305f\u554f\u984c\u89e3\u6c7a\u30fb\u65b0\u3057\u3044\u30a2\u30a4\u30c7\u30a2\u306e\u63a2\u6c42\u3002</p> <p>\u8a73\u7d30\u306f\u3053\u3061\u3089\u3002</p>"},{"location":"#follow-me","title":"Follow Me","text":"<ul> <li>: GitHub</li> <li>: X(Twitter)</li> <li>Qiita</li> <li>note</li> </ul>"},{"location":"about/","title":"Profile","text":"<p>Hi, I am hmasdev \ud83d\udc4b</p> enja <p>I am a data scientist and a machine learning engineer in Tokyo, Japan, specializing in using data science and AI to solve problems. I am passionate about solving problems and exploring new ideas.</p> <p>Through this blog, I will share my knowledge and thoughts about</p> <ul> <li>\ud83d\udcca\u3000data science;</li> <li>\ud83e\udd16\u3000AI;</li> <li>  programming;</li> <li>\u2328\ufe0f creating keyboards.</li> </ul> <p>I hope to contribute to the community, spark curiosity, and inspire others to learn and grow with me.</p> <p>\u6771\u4eac\u5728\u4f4f\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u30fb\u6a5f\u68b0\u5b66\u7fd2\u30a8\u30f3\u30b8\u30cb\u30a2\u3002</p> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3084AI\u3092\u7528\u3044\u305f\u8ab2\u984c\u89e3\u6c7a\u3001\u65b0\u3057\u3044\u30a2\u30a4\u30c7\u30a2\u306e\u63a2\u6c42\u306b\u596e\u95d8\u3002</p> <p>\u3053\u306e\u30d6\u30ed\u30b0\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u30c6\u30fc\u30de\u306e\u3064\u3044\u3066\u30e1\u30e2\u30fb\u5fd8\u5099\u9332\u3092\u57f7\u7b46\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <ul> <li>\ud83d\udcca \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3001</li> <li>\ud83e\udd16 AI\u3001</li> <li>  \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3001</li> <li>\u81ea\u4f5c\u30ad\u30fc\u30dc\u30fc\u30c9 \u2328\ufe0f\u3002</li> </ul> <p>\u6280\u8853\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3078\u306e\u8ca2\u732e\u3057\u3001\u4ed6\u306e\u65b9\u3005\u304c\u5b66\u3073\u3001\u6210\u9577\u306e\u6a5f\u4f1a\u306b\u7e4b\u304c\u308c\u3070...\uff01</p>"},{"location":"about/#mission-vision-and-values","title":"Mission, Vision, and Values","text":"enja <p>\"The most fulfilling thing is to do what you really want to do\"</p> <p>I believe that everyone has something they want to do. However, there are times when they give up on it for some reason. I think that is a very unfortunate thing.</p> <p>I aim to help people realize what they want to do by providing technical knowledge.</p> <p>\"\u672c\u5f53\u306b\u3084\u308a\u305f\u3044\u3053\u3068\u3092\u3084\u308b\u306e\u304c\u4e00\u756a\"</p> <p>\u8ab0\u3057\u3082\u3084\u308a\u305f\u3044\u3053\u3068\u304c\u3042\u308b\u3002\u3057\u304b\u3057\u3001\u4f55\u3089\u304b\u306e\u7406\u7531\u3067\u305d\u308c\u3092\u8ae6\u3081\u3066\u3057\u307e\u3046\u3053\u3068\u304c\u3042\u308b\u3002\u305d\u308c\u306f\u975e\u5e38\u306b\u6b8b\u5ff5\u306a\u3053\u3068\u3060\u3068\u601d\u3046\u3002</p> <p>\u79c1\u306f\u3001\u6280\u8853\u7684\u306a\u77e5\u898b\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3067\u3001\u3084\u308a\u305f\u3044\u3053\u3068\u3092\u5b9f\u73fe\u3059\u308b\u304a\u624b\u4f1d\u3044\u3092\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"about/#mission","title":"\ud83c\udfaf Mission","text":"enja <p>Providing impactful technical knowledge and tools to help people turn their ideas into reality.</p> <p>\u4eba\u3005\u304c\u30a2\u30a4\u30c7\u30a2\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306e\u6280\u8853\u7684\u306a\u77e5\u8b58\u3068\u30c4\u30fc\u30eb\u3092\u63d0\u4f9b\u3059\u308b\u3002</p>"},{"location":"about/#vision","title":"\ud83d\udd2d Vision","text":"enja <p>A world where everyone has the tools and confidence to pursue their passions and do what they truly want to do</p> <p>\u8ab0\u3082\u304c\u672c\u5f53\u306b\u3084\u308a\u305f\u3044\u3053\u3068\u3092\u8ffd\u6c42\u3057\u3001\u305d\u308c\u3092\u5b9f\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u4e16\u754c</p>"},{"location":"about/#values","title":"\u2728 Values","text":"enja <ul> <li>\"Be Curious\": Curiosity fuels motivation and creativity, driving continuous learning and discovery;</li> <li>\"Try First\": Action beats perfection \u2014 doing first, learning from failures, and iterating on learnings lead to real progress;</li> <li>\"Discuss a Lot\": Collaborative discussions spark insights and breakthroughs.</li> </ul> <ul> <li>\"\u597d\u5947\u5fc3\u65fa\u76db\u305f\u308c\": \u597d\u5947\u5fc3\u306f\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3068\u5275\u9020\u6027\u3001\u7d99\u7d9a\u7684\u306a\u5b66\u3073\u3068\u767a\u898b\u306e\u539f\u52d5\u529b;</li> <li>\"\u307e\u305a\u306f\u3084\u3063\u3066\u307f\u308b\": \u5b8c\u74a7\u3092\u76ee\u6307\u3059\u3088\u308a\u3082\u5148\u306b\u884c\u52d5\u3059\u308b\u65b9\u304c\u52dd\u308b \u2014 \u5931\u6557\u304b\u3089\u5b66\u3073\u3001\u5b66\u3073\u3092\u6d3b\u304b\u3059\u53cd\u5fa9\u304c\u672c\u5f53\u306e\u9032\u6b69\u306b;</li> <li>\"\u6d3b\u767a\u306b\u610f\u898b\u4ea4\u63db\u3057\u3088\u3046\": \u6d3b\u767a\u306a\u610f\u898b\u4ea4\u63db\u306f\u591a\u304f\u306e\u6c17\u3065\u304d\u3092\u3082\u305f\u3089\u3057\u3001\u30d6\u30ec\u30fc\u30af\u30b9\u30eb\u30fc\u306e\u7a2e\u3068\u306a\u308b\u3002</li> </ul>"},{"location":"about/#biography","title":"\ud83d\udcbc Biography","text":""},{"location":"about/#work-experience","title":"\ud83c\udfe2 Work Experience","text":"enja <ul> <li>Data Scientist and Machine Learning Engineer in a financial company, Tokyo, Japan (2019/04 - present)</li> </ul> <ul> <li>\u91d1\u878d\u6a5f\u95a2\u306b\u304a\u3051\u308b\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u30fb\u6a5f\u68b0\u5b66\u7fd2\u30a8\u30f3\u30b8\u30cb\u30a2 (2019/04 - \u73fe\u5728)</li> </ul>"},{"location":"about/#education","title":"\ud83c\udf93 Education","text":"<p>\ud83d\udea7 TBD \ud83d\udea7</p>"},{"location":"articles/","title":"Articles","text":"<p>Recent articles are listed below.</p>"},{"location":"articles/externals/","title":"External Articles","text":""},{"location":"articles/externals/#qiita","title":"qiita","text":"DNN\u3092\u4f7f\u308f\u305a\u306b\u62e1\u6563\u30e2\u30c7\u30eb\u3067\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u305f\u304f\u306a\u3063\u305f\u306e\u3067\u3084\u3063\u3066\u307f\u305f - Qiita <p>Stable Diffusion \u306a\u3069\u306e\u753b\u50cf\u751f\u6210\u30e2\u30c7\u30eb\u3084\u305d\u308c\u3089\u306b\u57fa\u3065\u304f\u30b5\u30fc\u30d3\u30b9\u304c\u6d41\u884c\u3057\u59cb\u3081\u3066\u4e45\u3057\u304f\u306a\u308a\u307e\u3057\u305f\ud83e\udd16\u304c...\u304a\u6065\u305a\u304b\u3057\u3044\u3053\u3068\u306b\u79c1\u306f\u30d5\u30ef\u30c3\u3068\u3057\u304b\u62e1\u6563\u30e2\u30c7\u30eb\uff08\u30b9\u30b3\u30a2\u30e2\u30c7\u30eb\u3084\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u62e1\u6563\u78ba\u2026</p> AutoGen \u3092\u4f7f\u3063\u3066\u4eba\u72fc\u3092\u3084\u3063\u3066\u307f\u305f\uff08\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\uff09 - Qiita <p>\u6982\u8981AutoGen\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066\u3001\u4eba\u72fc\u30b2\u30fc\u30e0\u3092\u884c\u3046\u30a2\u30d7\u30ea\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\u3092\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u3002AutoGen\u306f\u8907\u6570\u306eLarge Language Model (LLM) \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u4e92\u3044\u306b\u2026</p> \u5b9f\u884c\u4f8b\u3067\u7406\u89e3\u3059\u308b `Runnable` \u306e\u7d99\u627f\u8005\u305f\u3061 in `langchain` - Qiita <p>\u6982\u8981\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001LLM\uff08Large Language Models\uff09\u3092\u6d3b\u7528\u3057\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u958b\u767a\u306e\u305f\u3081\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3001langchain\u306b\u304a\u3051\u308bRunnable\u30af\u30e9\u30b9\u3092\u7d99\u627f\u3057\u3066\u3044\u308b\u30af\u30e9\u30b9\u306b\u3064\u2026</p> \u5b9f\u884c\u4f8b\u3067\u7406\u89e3\u3059\u308b `Runnable` in `langchain` - Qiita <p>\u6982\u8981\u672c\u8a18\u4e8b\u306f\u3001langchain \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u5185\u306e Runnable \u30af\u30e9\u30b9\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u305d\u306e\u7406\u89e3\u3068\u6d3b\u7528\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002langchain \u306f\u3001OpenAI\u3084Gemini\u306a\u3069\u306e\u4e00\u822c\u7684\u306b\u77e5\u3089\u308c\u2026</p>"},{"location":"articles/externals/#note","title":"note","text":"\u81ea\u30ad\u6cbc\u306b\u5165\u9580\u3057\u3066\u304b\u30898\u30ab\u6708~1\u5e74\u3092\u632f\u308a\u8fd4\u3063\u3066\u307f\u305f\uff5chmasdev <p>\u524d\u56de\u306e\u6295\u7a3f\u304c\u81ea\u30ad\u6d3b\u8a18\u9332\u3068\u3057\u3066\u3061\u3087\u3046\u3069\u826f\u304b\u3063\u305f\u305f\u3081\u7d9a\u7de8\u3067\u3059\ud83c\udf89  \u524d\u56de\u8a18\u4e8b\u2193\u2193   \u672c\u8a18\u4e8b\u3067\u306f\u79c1\u304c\u81ea\u4f5c\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5165\u9580\u3057\u3066\u304b\u3089\u7d04\u534a\u5e74~1\u5e74\u3067\u4f55\u3092\u3084\u3063\u3066\u304d\u305f\u306e\u304b\u3092\u632f\u308a\u8fd4\u308a\u305f\u3044\u3068\u601d\u3044\u307e\u3059\ud83d\udc4f\u307e\u3060\u5165\u9580\u3055\u308c\u3066\u3044\u306a\u3044\u65b9\u3001\u305c\u3072\u9053\u3057\u308b\u3079\u3068\u3057\u3066\u3054\u53c2\u8003\u306b\u306a\u308c\u3070   \u3084\u3063\u305f\u3053\u3068  \u524d\u56de\u306e\u3084\u308a\u305f\u3044\u3053\u3068\u306e\u30ea\u30b9\u30c8\u3068\u53d6\u308a\u7d44\u307f\u72b6\u6cc1    \u30ad\u30fc\u30b9\u30a4\u30c3\u30c1\u6cbc\uff0830g\u4ee5\u4e0b\u6311\u6226\uff09\u2192 28g \u306b\u6311\u6226\ud83d\ude0a    https://x.com/hmdev3/status/1797114256539685249      \u30b8\u30e7\u30a4\u30b9\u30c6\u30a3\u30c3\u30af \u2192 ckb4\u3067\u5165\u9580\uff01\ud83d\ude0a    \u7121\u7dda\u5316 \u2192 \u672a\u7740\u624b\ud83d\ude35    QMK\u4ee5\u5916\u306e\u30d5\u30a1\u30fc\u30e0\u30a6\u30a7\u30a2\uff08ZMK\uff09\u2192 \u672a\u7740\u624b</p> \u81ea\u30ad\u6cbc\u306b\u5165\u9580\u3057\u3066\u304b\u30897\u30ab\u6708\u3092\u632f\u308a\u8fd4\u3063\u3066\u307f\u305f\uff5chmasdev <p>\u8a18\u5ff5\u3059\u3079\u304d\u521d\u6295\u7a3f\ud83c\udf89\uff08\u3044\u307e\u3055\u3089\ud83e\udd14\uff09  \u90fd\u5185\u3067\u6a5f\u68b0\u5b66\u7fd2\u3046\u3093\u3061\u3083\u3089\u304b\u3093\u3061\u3083\u3089\u3084\u3063\u3066\u3044\u308bhmasdev\uff08X \u2192 @hmdev3, github \u2192 @hmasdev\uff09\u3068\u3044\u3044\u307e\u3059\ud83d\ude47\u200d\u2642\ufe0f note\u3067\u306f X \u3068\u540c\u3058\u3088\u3046\u306b python/AI\u30fb\u6a5f\u68b0\u5b66\u7fd2/\u81ea\u4f5c\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u3064\u3044\u3066\u6295\u7a3f\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\uff01  \u672c\u8a18\u4e8b\u3067\u306f\u79c1\u304c\u81ea\u4f5c\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5165\u9580\u3057\u3066\u304b\u3089\u534a\u5e74\u3061\u3087\u3063\u3068\u3067\u4f55\u3092\u4f5c\u3063\u3066\u304d\u305f\u306e\u304b\u3092\u632f\u308a\u8fd4\u308a\u305f\u3044\u3068\u601d\u3044\u307e\u3059\ud83d\udc4f\u307e\u3060\u5165\u9580\u3055\u308c\u3066\u3044\u306a\u3044\u65b9\u3001\u305c\u3072\u9053\u3057\u308b\u3079\u3068\u3057\u3066\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff01\u81ea\u30ad\u754c\u9688\u306e\u5927\u5148\u8f29\u65b9\u3001\u82e5\u8f29\u8005\u3067\u3059\u304c\u6b53\u8fce\u3057\u3066\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5b09\u3057\u3044\u3067\u3059\ud83e\udd7a   \u81ea\u30ad\u6cbc\u306b\u5165\u9580\u3057\u305f\u304d\u3063\u304b\u3051  \u79c1\u306f\u5927\u5b66(\u9662)\u6642\u4ee3\u304b\u3089\u305a\u3063\u3068HHKB</p>"},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/","title":"[Denoising Score Matching] Bayesian Interpretation of Why the Noise Prediction Model (Denoising Model) Can Estimate the Score Function","text":"<p>Warning</p> enja <p>This article is partially generated by generative AI. Before using the content, please verify it by yourself. If you notice any errors or inappropriate expressions, please let me know. Thank you.</p> <p>\u3053\u306e\u8a18\u4e8b\u306e\u57f7\u7b46\u306b\u3042\u305f\u308a\u4e00\u90e8\u751f\u6210AI\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002 \u5f53\u65b9\u3067\u3082\u5185\u5bb9\u306e\u78ba\u8a8d\u306f\u3057\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u5185\u5bb9\u3092\u9d5c\u5451\u307f\u306b\u305b\u305a\u3001\u3054\u81ea\u8eab\u3067\u78ba\u8a8d\u306e\u4e0a\u3054\u6d3b\u7528\u304f\u3060\u3055\u3044\u3002 \u3082\u3057\u3001\u8aa4\u308a\u3084\u4e0d\u9069\u5207\u306a\u8868\u73fe\u306a\u3069\u304a\u6c17\u3065\u304d\u306e\u70b9\u304c\u3054\u3056\u3044\u307e\u3057\u305f\u3089\u3001\u304a\u624b\u6570\u3067\u3059\u304c\u304a\u77e5\u3089\u305b\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\u3002</p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#introduction","title":"Introduction","text":"enja <p>It has been a long time since image generation models such as Stable Diffuion and Dall-e were announced. The boom of generative AI is still ongoing.</p> <p>In this article, we will use Bayesian estimation to interpret why the noise prediction model (denoising model) can estimate the score function to deepen our understanding of denoising score matching, one of the diffusion models.</p> <p>Stable Diffuion, Dall-e \u306a\u3069\u306e\u753b\u50cf\u751f\u6210\u30e2\u30c7\u30eb\u304c\u767a\u8868\u3055\u308c\u3066\u4e45\u3057\u304f\u306a\u308a\u307e\u3057\u305f\u3002\u4eca\u3082\u307e\u3060\u307e\u3060\u751f\u6210AI\u306e\u30d6\u30fc\u30e0\u306f\u7d9a\u3044\u3066\u3044\u307e\u3059\u3002</p> <p>\u672c\u7a3f\u3067\u306f\u3001\u62e1\u6563\u30e2\u30c7\u30eb\u306e\u4e00\u3064\u3067\u3042\u308b\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306e\u7406\u89e3\u3092\u6df1\u3081\u308b\u3079\u304f\u3001\u30ce\u30a4\u30ba\u4e88\u6e2c\u30e2\u30c7\u30eb\uff08\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30e2\u30c7\u30eb\uff09\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u7406\u7531\u306b\u3064\u3044\u3066\u3001\u30d9\u30a4\u30ba\u63a8\u5b9a\u3092\u4f7f\u3063\u305f\u89e3\u91c8\u3092\u884c\u3044\u307e\u3059\u3002</p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#target-audience","title":"Target Audience","text":"enja <ul> <li>People studying diffusion models</li> <li>People who have studied diffusion models but are not convinced that noise can be predicted</li> </ul> <ul> <li>\u62e1\u6563\u30e2\u30c7\u30eb\u3092\u52c9\u5f37\u4e2d\u306e\u4eba</li> <li>\u62e1\u6563\u30e2\u30c7\u30eb\u3092\u52c9\u5f37\u3057\u305f\u304c\u30ce\u30a4\u30ba\u3092\u4e88\u6e2c\u3059\u308b\u3053\u3068\u306b\u7d0d\u5f97\u304c\u3044\u304b\u306a\u3044\u4eba</li> </ul>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#what-is-denoising-score-matching","title":"What is Denoising Score Matching?","text":"enja <p>Let us consider the problem of estimating the probability distribution \\(p(x)\\) of \\(x \\in \\mathbb{R}^d\\) from a finite number of observations \\(\\{x_n\\}_{n=1}^N\\). \"Denoising Score Matching\" is one of the methods to solve this problem, which estimates the score function \\(s(x):=\\nabla_x \\log p(x)\\) of the probability distribution \\(p(x)\\), and estimates the probability distribution \\(p(x)\\).</p> <p>\\(x \\in \\mathbb{R}^d\\) \u3068\u3057\u3001\\(x\\) \u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03\uff08\u672a\u77e5\uff09\\(p(x)\\) \u3092\u6709\u9650\u500b\u306e\u89b3\u6e2c\u5024 \\(\\{x_n\\}_{n=1}^N\\) \u304b\u3089\u63a8\u5b9a\u3059\u308b\u3068\u3044\u3046\u554f\u984c\u3092\u8003\u3048\u307e\u3057\u3087\u3046\u3002 \u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306f\u3001\u3053\u306e\u554f\u984c\u3092\u89e3\u304f\u305f\u3081\u306e\u624b\u6cd5\u306e1\u3064\u3067\u3001\u78ba\u7387\u5206\u5e03 \\(p(x)\\) \u306e\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(x):=\\nabla_x \\log p(x)\\) \u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u78ba\u7387\u5206\u5e03 \\(p(x)\\) \u3092\u63a8\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002</p> <p>Note</p> enja <p>It should be noted that even if the score function \\(s(x):=\\nabla_x \\log p(x)\\) can be estimated, it is difficult to estimate the probability distribution \\(p(x)\\) itself.</p> <p>This is because in order to calculate \\(p(x)\\) from \\(s(x)\\), it is necessary to determine the normalization constant, i.e. the partition function \\(Z\\), by integration, which is generally difficult to calculate.</p> <p>However, although it is difficult to estimate \\(p(x)\\) itself, it is possible to sample from \\(p(x)\\) using the score function \\(s(x)\\) by using the following Langevin-MonteCarlo method.</p> \\[ x_{t+1} = x_t + \\alpha s(x) + \\sqrt{2\\alpha} \\xi \\] <p>where \\(\\xi \\sim \\mathcal{N}(0, I_d)\\) (\\(I_d\\) is the \\(d\\)-dimensional identity matrix).</p> <p>\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(x):=\\nabla_x \\log p(x)\\) \u304c\u63a8\u5b9a\u3067\u304d\u305f\u3068\u3057\u3066\u3082 \\(p(x)\\) \u81ea\u4f53\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3053\u3068\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\u306a\u305c\u306a\u3089\u3070 \\(s(x)\\) \u304b\u3089 \\(p(x)\\) \u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u898f\u683c\u5316\u5b9a\u6570\uff08\u3064\u307e\u308a\u5206\u914d\u95a2\u6570 \\(Z\\)\uff09\u3092\u7a4d\u5206\u306b\u3088\u308a\u6c42\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u4e00\u822c\u306b\u306f\u8a08\u7b97\u304c\u56f0\u96e3\u3060\u304b\u3089\u3067\u3059\u3002</p> <p>\u3057\u304b\u3057\u306a\u304c\u3089\u3001\\(p(x)\\) \u81ea\u4f53\u306e\u63a8\u5b9a\u306f\u56f0\u96e3\u3067\u3059\u304c\u3001\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(x)\\) \u3092\u7528\u3044\u3066 \\(p(x)\\) \u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u306f\u4e0b\u8a18\u306e Langevin-MonteCarlo\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u53ef\u80fd\u3067\u3059\u3002</p> \\[ x_{t+1} = x_t + \\alpha s(x) + \\sqrt{2\\alpha} \\xi \\] <p>\u305f\u3060\u3057\u3001\\(\\xi \\sim \\mathcal{N}(0, I_d)\\) (\\(I_d\\) \u306f \\(d\\) \u6b21\u5143\u306e\u5358\u4f4d\u884c\u5217) \u3067\u3059\u3002</p> enja <p>In denoising score matching, the score function \\(s(x)\\) is estimated by minimizing the following objective function with \\(s(x)\\) replaced by \\(s_\\theta(x)\\).</p> <p>\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u3067\u306f\u4e0b\u8a18\u306e\u76ee\u7684\u95a2\u6570\u3092\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(x)\\) \u3092 \\(s_\\theta(x)\\) \u3067\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> \\[ J_{DSM_{p_\\sigma}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2I_d), x\\sim p(x)}\\left[\\left\\| - \\frac{\\varepsilon}{\\sigma^2} - s_\\theta(x+\\varepsilon;\\sigma)\\right\\|^2\\right] \\] enja <p>Interpreting this equation literally, it means that \\(s_\\theta\\) is learned to predict (denoise) the noise \\(\\varepsilon\\) added to the original sample \\(x\\) from the noisy sample \\(x+\\varepsilon\\). However, this is actually known to be equivalent to minimizing the following objective function (Vincent, 2011)<sup>1</sup>:</p> <p>\u3053\u308c\u306f\u5f0f\u306e\u610f\u5473\u3092\u305d\u306e\u307e\u307e\u89e3\u91c8\u3059\u308b\u3068 $s_\\theta $ \u304c\u30ce\u30a4\u30ba\u304c\u4ed8\u4e0e\u3055\u308c\u305f\u30b5\u30f3\u30d7\u30eb \\(x+\\varepsilon\\) \u304b\u3089\u5143\u306e\u30b5\u30f3\u30d7\u30eb \\(x\\) \u306b\u52a0\u3048\u3089\u308c\u305f\u30ce\u30a4\u30ba \\(\\varepsilon\\) \u3092\u4e88\u6e2c\u3059\u308b\uff08\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\uff09\u3088\u3046\u306b\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u306f\u4e0b\u8a18\u3092\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3068\u7b49\u4fa1\u3067\u3042\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u307e\u3059(Vincent, 2011)<sup>1</sup>:</p> \\[ J_{ESM_{p_\\sigma}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{\\tilde{x}\\sim p_\\sigma(x)}\\left[\\left\\| \\nabla_{\\tilde{x}} \\log p_\\sigma(x) - s_\\theta(\\tilde{x};\\sigma)\\right\\|^2\\right], \\] enja <p>where \\(p_\\sigma(\\tilde{x}) = \\int p(x) \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I_d) dx\\) and \\(p_\\sigma(\\tilde{x})\\) is the perturbed distribution of \\(p(x)\\).</p> <p>The \\(s_\\theta(\\tilde{x};\\sigma)\\) learned by this minimization approximates the score function of \\(p_\\sigma(\\tilde{x})\\), especially when \\(\\sigma\\) is sufficiently small, it approximates the score function of \\(p(x)\\).</p> <p>So far, we have seen denoising score matching, but it is difficult to understand intuitively whether noise prediction and score function estimation are equivalent (at least for me). Okanohara(2023)<sup>3</sup> used a figure of Vincent(2010)<sup>2</sup> to explain the intuitive meaning of denoising score matching.</p> <p>Adding noise causes the probability to jump to low probability regions. The average denoising in various directions becomes the perpendicular to the direction of high probability.</p> <p>\u305f\u3060\u3057\u3001\\(p_\\sigma(\\tilde{x}) = \\int p(x) \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I_d) dx\\) \u3067 \\(p_\\sigma(\\tilde{x})\\) \u306f \\(p(x)\\) \u3092\u5c11\u3057\u307c\u3084\u304b\u3057\u305f\u6442\u52d5\u5f8c\u5206\u5e03\u3067\u3059\u3002</p> <p>\u3053\u306e\u6700\u5c0f\u5316\u306b\u3088\u308a\u5b66\u7fd2\u3055\u308c\u308b \\(s_\\theta(\\tilde{x};\\sigma)\\) \u306f \\(p_\\sigma(\\tilde{x})\\) \u306e\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u8fd1\u4f3c\u3001\u7279\u306b \\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306f \\(p(x)\\) \u306e\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u8fd1\u4f3c\u3057\u307e\u3059\u3002</p> <p>\u3055\u3066\u3001\u3053\u3053\u307e\u3067\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u3064\u3044\u3066\u898b\u3066\u304d\u307e\u3057\u305f\u304c\u3001\u30ce\u30a4\u30ba\u306e\u4e88\u6e2c\u3068\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u304c\u7b49\u4fa1\u3067\u3042\u308b\u306e\u304b\u3001\u76f4\u611f\u7684\u306b\u306f\u7406\u89e3\u3057\u304c\u305f\u3044\u3067\u3059\uff08\u5c11\u306a\u304f\u3068\u3082\u79c1\u306b\u306f\uff09\u3002\u5ca1\u91ce\u539f\u3055\u3093\u306f<sup>3</sup>\u306b\u304a\u3044\u3066Vincent(2010)<sup>2</sup>\u306e\u56f3\u3092\u7528\u3044\u3066\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306e\u76f4\u611f\u7684\u306a\u610f\u5473\u3092\u4e0b\u8a18\u306e\u3088\u3046\u306b\u8aac\u660e\u3057\u3066\u3044\u307e\u3059\uff1a</p> <p>\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u308b\u3068\u78ba\u7387\u304c\u4f4e\u3044\u9818\u57df\u3078\u98db\u3073\u51fa\u3059\u3002\u69d8\u3005\u306a\u65b9\u5411\u3078\u306e\u30c7\u30ce\u30a4\u30b8\u30f3\u306e\u5e73\u5747\u306f\u78ba\u7387\u304c\u9ad8\u3044\u65b9\u5411\u3078\u306e\u5782\u7dda\u3068\u306a\u308b</p> <p></p> enja <p>However, it is still difficult to understand why the noise prediction model becomes the score function.</p> <p>In the next section, we will consider a simple case and try to intuitively understand why denoising becomes an estimation of the score function.</p> <p>\u3084\u306f\u308a\u306a\u305c\u30ce\u30a4\u30ba\u4e88\u6e2c\u30e2\u30c7\u30eb\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306b\u306a\u308b\u306e\u304b\u72d0\u306b\u3064\u307e\u307e\u308c\u305f\u3088\u3046\u306a\u6c17\u6301\u3061\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u305d\u3053\u3067\u6b21\u7bc0\u3067\u306f\u7c21\u5358\u306a\u30b1\u30fc\u30b9\u3092\u8003\u3048\u3066\u3001\u306a\u305c\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u306e\u304b\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002</p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function","title":"Bayesian Interpretation of Why the Noise Prediction Model (Denoising Model) Can Estimate the Score Function","text":"enja <p>Here, we consider a simple case to intuitively understand why denoising becomes an estimation of the score function.</p> <p>First, to consider the denoising problem, we consider the following naive one-dimensional case:</p> <p>\u3055\u3066\u3001\u3053\u3053\u3067\u306f\u7c21\u5358\u306a\u30b1\u30fc\u30b9\u3092\u8003\u3048\u3066\u3001\u306a\u305c\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u306e\u304b\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u306e\u554f\u984c\u3092\u8003\u3048\u308b\u305f\u3081\u306b\u6b21\u306e\u7d20\u6734\u306a1\u6b21\u5143\u306e\u30b1\u30fc\u30b9\u3092\u8003\u3048\u307e\u3059:</p> \\[ \\tilde{x} = x + \\varepsilon, \\] enja <p>where \\(x \\in \\mathbb{R} \\sim p(x)=:f(x)\\), \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), and \\(x \\perp \\varepsilon\\).</p> <p>This represents the process of adding noise to the original sample in denoising score matching. The problem we want to consider here is the denoising problem, where we estimate \\(\\varepsilon\\) given \\(\\tilde{x}\\). We will see that solving this problem leads to estimating the score function of \\(p(x)\\). Let's scale \\(\\varepsilon\\) by \\(e = \\varepsilon / \\sigma^2\\). Then, the above case can be rewritten as follows:</p> <p>\u305f\u3060\u3057\u3001\\(x \\in \\mathbb{R} \\sim p(x)=:f(x)\\)\u3001\\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), \\(x \\perp \\varepsilon\\)\u3002</p> <p>\u3053\u308c\u306f\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b5\u30f3\u30d7\u30eb\u306b\u30ce\u30a4\u30ba\u3092\u4ed8\u4e0e\u3059\u308b\u904e\u7a0b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u3053\u3067\u8003\u3048\u305f\u3044\u554f\u984c\u306f\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u3001\\(\\tilde{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067 \\(\\varepsilon\\) \u3092\u63a8\u5b9a\u3059\u308b\u554f\u984c\u3067\u3059\u3002 \u3053\u306e\u554f\u984c\u3092\u89e3\u304f\u3053\u3068\u304c \\(p(x)\\) \u306e\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3053\u3068\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002 \\(\\varepsilon\\) \u3092 \\(e = \\varepsilon / \\sigma^2\\) \u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u3059\u308b\u3068\u3001\u4e0a\u8a18\u306e\u30b1\u30fc\u30b9\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u66f8\u304d\u63db\u3048\u3089\u308c\u307e\u3059:</p> \\[ \\tilde{x} = x + \\sigma^2 e, \\] enja <p>where \\(e \\sim \\mathcal{N}(0, \\sigma^{-2})\\), and \\(x \\perp e\\).</p> <p>Now, let's consider the problem of estimating \\(e\\) given \\(\\tilde{x}\\). Using the Bayes' theorem with \\(x \\sim p(x)\\) and \\(\\tilde{x} = x + \\sigma^2 e\\), the posterior distribution of \\(e\\) is expressed as follows.</p> <p>\u305f\u3060\u3057\u3001\\(e \\sim \\mathcal{N}(0, \\sigma^{-2})\\), \\(x \\perp e\\)\u3002</p> <p>\u3055\u3066\u3001\u3053\u306e\u3068\u304d\u3001\\(\\tilde{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067 \\(e\\) \u3092\u63a8\u5b9a\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002 \\(x \\sim p(x)\\) \u304a\u3088\u3073 \\(\\tilde{x} = x + \\sigma^2 e\\) \u306b\u6ce8\u610f\u3057\u3066\u30d9\u30a4\u30ba\u306e\u5b9a\u7406\u3092\u4f7f\u3046\u3068 \\(e\\) \u306e\u4e8b\u5f8c\u5206\u5e03\u306f\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[ \\begin{matrix} p(e|\\tilde{x}) &amp; \\propto &amp; p(\\tilde{x}|e)p(e) \\\\ &amp; = &amp; f(\\tilde{x} - \\sigma^2 e)\\mathcal{N}(e|0, \\sigma^{-2}) \\\\ \\end{matrix} \\] enja <p>Now, let's use this posterior distribution to perform MAP estimation of \\(e\\). The MAP estimation of \\(e\\) can be calculated as follows.</p> <p>\u3055\u3066\u3001\u3053\u306e\u4e8b\u5f8c\u5206\u5e03\u3092\u4f7f\u3063\u3066 MAP \u63a8\u5b9a\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002 \\(e\\) \u306eMAP\u63a8\u5b9a\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p> \\[ \\begin{matrix} \\hat{e} &amp; = &amp; \\arg\\max_e p(e|\\tilde{x}) \\\\ &amp;=&amp; \\arg\\max_e \\log p(e|\\tilde{x}) &amp; (\\because\\;\\log\\;{\\rm is}\\;{\\rm monotonically}\\;{\\rm increasing}) \\\\ &amp;=&amp; \\arg\\max_e \\log f(\\tilde{x} - \\sigma^2 e) + \\log \\mathcal{N}(e|0, \\sigma^{-2}) &amp; (\\because\\;{\\rm the}\\;{\\rm Bayes}\\;{\\rm rule}) \\\\ &amp;=&amp; \\arg\\max_e \\log f(\\tilde{x} - \\sigma^2 e) - \\frac{1}{2}\\sigma^2e^2 \\\\ \\end{matrix} \\] enja <p>Then, from the first-order condition of \\(\\hat{e}\\)'s optimality, the following equation holds.</p> <p>\u305d\u3057\u3066\u3001\\(\\hat{e}\\) \u306e\u6700\u9069\u6027\u306e1\u6b21\u306e\u6761\u4ef6\u3088\u308a\u3001 \u6b21\u306e\u5f0f\u304c\u6210\u308a\u7acb\u3061\u307e\u3059</p> \\[ \\begin{matrix} 0 &amp; = &amp; \\frac{\\rm d}{{\\rm d}e} \\left( \\log f(\\tilde{x} - \\sigma^2 e) - \\frac{1}{2}\\sigma^2e^2 \\right)|_{e=\\hat{e}} \\\\ &amp; = &amp; -\\sigma^2 \\frac{f^\\prime(\\tilde{x} - \\sigma^2 \\hat{e})}{f(\\tilde{x} - \\sigma^2 \\hat{e})} - \\sigma^2 \\hat{e} \\\\ \\end{matrix} \\] enja <p>Therefore, \\(\\hat{e}\\) is expressed as follows.</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\hat{e}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[ \\hat{e} = -\\frac{f^\\prime(\\tilde{x} - \\sigma^2 \\hat{e})}{f(\\tilde{x} - \\sigma^2 \\hat{e})} = - \\left(\\frac{\\rm d}{{\\rm d}x} \\log f\\right)(\\tilde{x} - \\sigma^2 \\hat{e}) = - s(\\tilde{x} - \\sigma^2 \\hat{e}), \\] enja <p>where \\(s(x)\\) is the score function of \\(p(x)=f(x)\\).</p> <p>Thus, it is understood that the MAP estimation of \\(e\\) in denoising is expressed using the score function. However, the above is MAP estimation. On the other hand, denoising score matching uses the least squares method. There is still a gap from this perspective.</p> <p>Next, let's consider how the posterior distribution \\(p(e|\\tilde{x})\\) in the case where \\(\\sigma\\) is sufficiently small can be approximated. Note that \\(e = \\mathcal{O}(\\sigma^{-1})\\) because \\(e \\sim \\mathcal{N}(0, \\sigma^{-2})\\). By performing a Taylor expansion for \\(\\log p(e|\\tilde{x}) = \\log f(\\tilde{x} - \\sigma^2 e) - \\frac{1}{2}\\sigma^2e^2 + {\\rm const.}\\), and approximating it as a quadratic function of \\(e\\) while ignoring \\(\\mathcal{O}(\\sigma^3)\\) (\\(\\sigma^6 e^3\\), etc.), we obtain the following result.</p> <p>\u305f\u3060\u3057\u3001\\(s(x)\\) \u306f \\(p(x)=f(x)\\) \u306e\u30b9\u30b3\u30a2\u95a2\u6570\u3067\u3059\u3002</p> <p>\u3055\u3066\u3001\u4ee5\u4e0a\u304b\u3089\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u306e \\(e\\) \u306e MAP \u63a8\u5b9a\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u7528\u3044\u3066\u8868\u73fe\u3055\u308c\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002 \u305f\u3060\u3001\u4e0a\u8a18\u306f MAP \u63a8\u5b9a\u3067\u3042\u308a\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u306f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3002\u3053\u306e\u89b3\u70b9\u3067\u307e\u3060\u30ae\u30e3\u30c3\u30d7\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u3053\u3053\u3067 \\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306e\u4e0a\u8a18\u306e\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u304c\u3069\u306e\u3088\u3046\u306b\u8fd1\u4f3c\u3067\u304d\u308b\u304b\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002 \\(e \\sim \\mathcal{N}(0, \\sigma^{-2})\\) \u3088\u308a \\(e = \\mathcal{O}(\\sigma^{-1})\\) \u306b\u3082\u6c17\u3092\u4ed8\u3051\u306a\u304c\u3089\u3001\\(\\log p(e|\\tilde{x}) = \\log f(\\tilde{x} - \\sigma^2 e) - \\frac{1}{2}\\sigma^2e^2 + {\\rm const.}\\) \u306b\u5bfe\u3057\u3066\u30c6\u30a4\u30e9\u30fc\u5c55\u958b\u3092\u884c\u3044\u3001\\(\\mathcal{O}(\\sigma^3)\\) (\\(\\sigma^6 e^3\\) \u306a\u3069) \u3092\u7121\u8996\u3057\u3064\u3064\u3001\\(e\\) \u306e\u4e8c\u6b21\u95a2\u6570\u3068\u3057\u3066\u8fd1\u4f3c\u3057\u3066\u307f\u308b\u3068\u3001\u4e0b\u8a18\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> \\[ \\log p(e|\\tilde{x}) \\approx \\log \\mathcal{N}\\left(e|-(\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x}), \\sigma^{-2}\\right) \\] Warning enja <p>The detailed derivation is as follows, but no certainty that it is correct.</p> <p>\u8a73\u7d30\u306a\u5c0e\u51fa\u306f\u4ee5\u4e0b\u3067\u3059\u304c\u3001\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306f\u4e0d\u78ba\u304b\u3067\u3059\u3002</p> \\[ \\begin{matrix} \\log p(e|\\tilde{x}) &amp; = &amp; \\log f(\\tilde{x} - \\sigma^2 e) - \\frac{1}{2}\\sigma^2e^2 + {\\rm const.}\\\\ &amp; = &amp; \\log f(\\tilde{x}) - (\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})\\sigma^2 e + \\frac{1}{2}(\\frac{\\rm d^2}{{\\rm d}x^2}\\log f)(\\tilde{x})\\sigma^4 e^2+ \\mathcal{O}(\\sigma^6e^3)- \\frac{1}{2}\\sigma^2e^2 + {\\rm const.} &amp; (\\because\\;{\\rm Taylor}\\;{\\rm expansion}, e=\\mathcal{O}(\\sigma^{-1}))\\\\ &amp; = &amp; - \\frac{1}{2}\\left(1-(\\frac{\\rm d^2}{{\\rm d}x^2}\\log f)(\\tilde{x})\\sigma^2\\right)\\sigma^2e^2 - (\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})\\sigma^2 e + \\mathcal{O}(\\sigma^6e^3) + {\\rm const.}(\\tilde{x}) \\\\ &amp; = &amp; - \\frac{1}{2}\\left(1-(\\frac{\\rm d^2}{{\\rm d}x^2}\\log f)(\\tilde{x})\\sigma^2\\right)\\sigma^2\\left(e + \\frac{(\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})}{1-(\\frac{\\rm d^2}{{\\rm d}x^2}\\log f)(\\tilde{x})\\sigma^2}\\right)^2 + \\mathcal{O}(\\sigma^2)+\\mathcal{O}(\\sigma^6e^3) + {\\rm const.} \\\\ &amp; = &amp; - \\frac{1}{2}\\left(\\sigma^2 + \\mathcal{O}(\\sigma^4)\\right)\\left(e + (\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})+\\mathcal{O}(\\sigma^2)\\right)^2 + \\mathcal{O}(\\sigma^2)+\\mathcal{O}(\\sigma^6e^3) + {\\rm const.} \\\\ &amp; = &amp; \\log \\mathcal{N}\\left(e|-(\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})+\\mathcal{O}(\\sigma^2), \\frac{\\sigma^{-2}}{1+\\mathcal{O}(\\sigma^2)}\\right) + \\mathcal{O}(\\sigma^6e^3) + {\\rm const.} \\\\ &amp; = &amp; \\log \\mathcal{N}\\left(e|-(\\frac{\\rm d}{{\\rm d}x}\\log f)(\\tilde{x})+\\mathcal{O}(\\sigma^2), \\sigma^{-2} + \\mathcal{O}(1)\\right) + \\mathcal{O}(\\sigma^6e^3) + {\\rm const.} \\\\ \\end{matrix} \\] enja <p>In other words, it can be confirmed that the posterior distribution $p(e|\\tilde{x}) in the case where \\(\\sigma\\) is sufficiently small can be approximated by a Gaussian distribution with the score function as the mean. As a result, it can be naturally interpreted that in denoising score matching, the score function can be estimated by estimating the noise using a least squares method. Furthermore, it can be understood that the weight for performing denoising score matching for various \\(\\sigma\\) simultaneously is $\\sigma^2, based on the fact that the variance of the Gaussian distribution is \\(\\sigma^{-2}\\). This is consistent with the selection of weights based on empirical results in (Y. Song and S. Ermon, 2019)<sup>4</sup>.</p> <p>Thus, it has been intuitively understood that estimating noise in denoising score matching becomes an estimation of the score function from the perspective of maximizing the posterior distribution \\(p(e|\\tilde{x})\\).</p> <p>In particular, when \\(\\sigma\\) is sufficiently small, it has been obtained that the least squares method in denoising score matching becomes a least squares method with the noise intensity \\(\\sigma^2\\) as the weight, based on the fact that the posterior distribution \\(p(e|\\tilde{x})\\) can be approximated by a Gaussian distribution with the score function as the mean and \\(\\sigma^{-2}\\) as the variance.</p> <p>\u3064\u307e\u308a\u3001\\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306e\u4e0a\u8a18\u306e\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u304c\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u5e73\u5747\u3068\u3059\u308b\u30ac\u30a6\u30b9\u5206\u5e03\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u7d50\u679c\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u306f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u3088\u308a\u3001\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3067\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3053\u3068\u304c\u81ea\u7136\u3068\u89e3\u91c8\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059 \u307e\u305f\u3001\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u5206\u6563\u304c \\(\\sigma^{-2}\\) \u3067\u3042\u308b\u3053\u3068\u304b\u3089\u3001\u69d8\u3005\u306a \\(\\sigma\\) \u306b\u5bfe\u3059\u308b\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u3092\u540c\u6642\u306b\u884c\u3046\u969b\u306e\u91cd\u307f\u304c \\(\\sigma^2\\) \u3067\u3042\u308b\u3053\u3068\u3082\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f Y. Song and S. Ermon(2019) \u306e\u7d4c\u9a13\u7684\u306a\u7d50\u679c\u306b\u57fa\u3065\u304f\u91cd\u307f\u306e\u9078\u629e\u3068\u3082\u5408\u81f4\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4ee5\u4e0a\u306b\u3088\u308a\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u3067\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u3001\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u306e\u6700\u5927\u5316\u306e\u89b3\u70b9\u3067\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u304c\u76f4\u611f\u7684\u306b\u7406\u89e3\u3067\u304d\u307e\u3057\u305f\u3002</p> <p>\u7279\u306b \\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306b\u306f\u3001\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u304c\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u5e73\u5747\u3001\\(\\sigma^{-2}\\)\u3092\u5206\u6563\u3068\u3059\u308b\u30ac\u30a6\u30b9\u5206\u5e03\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u304b\u3089\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3051\u308b\u6700\u5c0f\u4e8c\u4e57\u6cd5\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3001\u30ce\u30a4\u30ba\u5f37\u5ea6\\(\\sigma^2\\)\u3092\u91cd\u307f\u3068\u3057\u305f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3068\u306a\u308b\u3068\u3044\u3046\u63cf\u50cf\u304c\u5f97\u3089\u308c\u307e\u3057\u305f\u3002</p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#numerical-verification-of-the-interpretation","title":"Numerical Verification of the Interpretation","text":"enja <p>In this section, we will numerically examine that the posterior distribution of noise seen above can be approximated by a Gaussian distribution with the negative of the score function as the mean.</p> <p>\u672c\u7bc0\u3067\u306f\u4e0a\u8a18\u3067\u898b\u305f\u30ce\u30a4\u30ba\u306e\u4e8b\u5f8c\u5206\u5e03\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u8ca0\u3092\u5e73\u5747\u5024\u3068\u3059\u308b\u30ac\u30a6\u30b9\u5206\u5e03\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u3092\u6570\u5024\u7684\u306b\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>Warning</p> enja <p>The code provided in this article is intended to demonstrate the operation in a specific environment and under specific conditions, and may not work the same in all environments or cases. Also, software updates and compatibility issues may occur over time, so the code provided may not be up to date. Use of this code is at the reader's own risk. Before executing, make sure you understand the code well and adapt it to your environment as needed. Also, don't forget to back up as much as possible. If you have any questions or suggestions for improvement regarding the content or code of this article, please let us know in the comments or on social media.</p> <p>\u672c\u8a18\u4e8b\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30c9\u306f\u3001\u7279\u5b9a\u306e\u74b0\u5883\u3084\u6761\u4ef6\u4e0b\u3067\u306e\u52d5\u4f5c\u3092\u793a\u3059\u3082\u306e\u3067\u3042\u308a\u3001\u5168\u3066\u306e\u74b0\u5883\u3084\u30b1\u30fc\u30b9\u3067\u540c\u69d8\u306b\u6a5f\u80fd\u3059\u308b\u3068\u306f\u9650\u308a\u307e\u305b\u3093\u3002\u307e\u305f\u3001\u6642\u9593\u306e\u7d4c\u904e\u3068\u3068\u3082\u306b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u66f4\u65b0\u3084\u4e92\u63db\u6027\u306e\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30c9\u304c\u6700\u65b0\u306e\u72b6\u614b\u3067\u3042\u308b\u3068\u306f\u9650\u308a\u307e\u305b\u3093\u3002\u672c\u30b3\u30fc\u30c9\u306e\u4f7f\u7528\u306f\u3001\u8aad\u8005\u306e\u8cac\u4efb\u306b\u304a\u3044\u3066\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\u5b9f\u884c\u3059\u308b\u524d\u306b\u3001\u30b3\u30fc\u30c9\u3092\u3088\u304f\u7406\u89e3\u3057\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u74b0\u5883\u306b\u9069\u5408\u3055\u305b\u308b\u3053\u3068\u3092\u63a8\u5968\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u53ef\u80fd\u306a\u9650\u308a\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3092\u53d6\u308b\u3053\u3068\u3092\u5fd8\u308c\u306a\u3044\u3067\u304f\u3060\u3055\u3044\u3002\u672c\u8a18\u4e8b\u306e\u5185\u5bb9\u3084\u30b3\u30fc\u30c9\u306b\u95a2\u3059\u308b\u8cea\u554f\u3084\u6539\u5584\u63d0\u6848\u304c\u3042\u308c\u3070\u3001\u30b3\u30e1\u30f3\u30c8\u6b04\u3084\u30bd\u30fc\u30b7\u30e3\u30eb\u30e1\u30c7\u30a3\u30a2\u3092\u901a\u3058\u3066\u304a\u77e5\u3089\u305b\u304f\u3060\u3055\u3044\u3002</p> <pre><code>!python -V\n# %pip intall numpy==2.1.3 sympy==1.13.3 matplotlib==3.9.2 pandas==2.2.3 scipy==1.14.1\n</code></pre> <pre><code>Python 3.11.5\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import kstest, norm\nimport sympy\n\n# fix seed\nnp.random.seed(42)\n\n# symbols for sympy\nx = sympy.symbols('x')\ne = sympy.symbols('e')\ntildex = sympy.symbols('tildex')  # tildex = x + e\nsigma = sympy.symbols('sigma')\n</code></pre>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#numerical-verification-using-analytically-calculated-posterior-distribution","title":"Numerical verification using analytically calculated posterior distribution","text":"enja <p>First, let's analytically calculate \\(p(e|\\tilde{x})\\) without generating data, and see that the score function can be approximated by MAP estimation of \\(e\\) when \\(\\sigma\\) is sufficiently small, and that \\(p(e|\\tilde{x})\\) can be approximated by \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\), where \\(s(x):=\\nabla_x \\log p(x)\\).</p> <p>\u307e\u305a\u306f\u30c7\u30fc\u30bf\u3092\u751f\u6210\u305b\u305a\u306b \\(p(e|\\tilde{x})\\) \u3092\u89e3\u6790\u7684\u306b\u8a08\u7b97\u3057\u3066\u3001\\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u6642\u306b MAP \u63a8\u5b9a\u3067\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u3001\u304a\u3088\u3073\u3001\\(p(e|\\tilde{x})\\) \u304c \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) \u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff08\u305f\u3060\u3057\u3001\\(s(x):=\\nabla_x \\log p(x)\\)\uff09\u3002</p>  The method of numerical verification by visualizing the posterior distribution $p(e|\\tilde{x})$  <pre><code>def plot_numerical_verification(\n    sympy_p_x,\n    sympy_p_e_posterior,\n    sympy_negative_score_function,\n    sigma_val: float = 0.1,\n    tildex_vals: np.ndarray = np.linspace(-5, 5, 128),\n    e_vals: np.ndarray = np.linspace(-5, 5, 256),\n    axs=None,\n    fig=None,\n    fewer_e_vals: np.ndarray = np.linspace(-3, 3, 7),\n):\n    tildex_mesh, e_mesh = np.meshgrid(tildex_vals, e_vals)\n    p_e_posterior_vals = sympy.lambdify((tildex, e, sigma), sympy_p_e_posterior, 'numpy')(tildex_mesh, e_mesh, sigma_val)\n\n    if axs is None:\n        fig, axs = plt.subplots(3, 1, figsize=(12, 16))\n\n    fig.suptitle(f'p(x)={sympy_p_x}')\n\n    ax = axs[0]\n    ax.set_title(f'posterior distribution p(e|tildex) without Z with sigma={sigma_val}')\n    ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals, levels=100, label='p(e|tildex) without Z')\n    ax.plot(tildex_vals, sympy.lambdify(x, sympy_negative_score_function)(tildex_vals), 'r', label=\"score function for \\\\tilde{x}\")\n    ax.set_xlabel('\\\\tilde{x}')\n    ax.set_ylabel('e')\n    ax.set_xlim(tildex_vals.min(), tildex_vals.max())\n    ax.set_ylim(e_vals.min(), e_vals.max())\n    ax.legend()\n\n    ax = axs[1]\n    ax.set_title(f'MAP Estimation of e vs Score Function with sigma={sigma_val}')\n    ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals/p_e_posterior_vals.max(axis=0), levels=100, label='p(e|\\\\tilde{x})/max_e p(e|\\\\tilde{x})')\n    ax.plot(tildex_vals, e_vals[np.argmax(p_e_posterior_vals, axis=0)], 'bo', label='argmax_e p(e|xi) for \\\\tilde{x}')\n    ax.plot(tildex_vals, sympy.lambdify(x, sympy_negative_score_function)(tildex_vals), 'r', label=\"score function for \\\\tilde{x}\")\n    ax.set_xlabel('\\\\tilde{x}')\n    ax.set_ylabel('e')\n    ax.set_xlim(tildex_vals.min(), tildex_vals.max())\n    ax.set_ylim(e_vals.min(), e_vals.max())\n    ax.legend()\n\n    ax = axs[2]\n    ax.set_title(f'Posterior Distribution are approximated by Normal Distribution with sigma={sigma_val}')\n    axd = ax.twinx()\n    for i, _tildex_val in enumerate(fewer_e_vals):\n        # plot p(e|tildex)\n        _p = sympy.lambdify((e, tildex, sigma), sympy_p_e_posterior)(e_vals, _tildex_val, sigma_val)\n        ax.plot(e_vals, _p/_p.max(), label=f'tildex={_tildex_val}', ls='-', marker='')\n        # plot N(e|score(tildex), \\sigma^{-2})\n        axd.plot(e_vals, norm.pdf(e_vals, loc=sympy.lambdify(x, sympy_negative_score_function)(_tildex_val), scale=1/sigma_val), ls='', marker='x', label=f'tildex={_tildex_val}', alpha=0.3)\n\n    ax.set_xlabel('e')\n    ax.set_ylabel('p(e|tilde_x) / max_e p(e|tilde_x)')\n    axd.set_ylabel('N(e|score(tilde_x), \\\\sigma^{-2})')\n    ax.legend(ncol=1, loc='upper left')\n    axd.legend(ncol=1, loc='lower right')\n</code></pre>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#case-1-the-standard-normal-distribution","title":"Case 1: The standard normal distribution","text":"enja <p>First, let's consider the case where \\(p(x)\\) is the standard normal distribution, i.e., \\(p(x) = \\mathcal{N}(x|0, 1)\\).</p> <p>\u307e\u305a\u306f \\(p(x)\\) \u304c\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u3001\u3059\u306a\u308f\u3061 \\(p(x) = \\mathcal{N}(x|0, 1)\\) \u306e\u5834\u5408\u3092\u8003\u3048\u307e\u3059\u3002</p> <pre><code># define pdf of x\nf = sympy.exp(-0.5*x**2) / sympy.sqrt(2 * sympy.pi)  # normal distribution\nf\n</code></pre> <p>\\(\\displaystyle \\frac{\\sqrt{2} e^{- 0.5 x^{2}}}{2 \\sqrt{\\pi}}\\)</p> enja <p>In the following code, we plot the posterior distribution \\(p(e|\\tilde{x})\\) (left), MAP estimation (center), and the posterior distribution \\(p(e|\\tilde{x})\\) and \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) (right) for $\\sigma = $ 0.1 (top), 0.5 (middle), and 1 (bottom).  As a result, we can see that as \\(\\sigma\\) becomes sufficiently small, the MAP estimation of \\(e\\), i.e., \\(\\arg\\max _e p(e|\\tilde{x})\\), approaches the score function \\(s(\\tilde{x})\\) (center). Also, we can see that as \\(\\sigma\\) becomes sufficiently small, the posterior distribution \\(p(e|\\tilde{x})\\) approaches \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) (right).</p> <p>\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u306f $\\sigma = $ 0.1\uff08\u4e0a\u6bb5\uff09, 0.5\uff08\u4e2d\u65ad\uff09, 1\uff08\u4e0b\u6bb5\uff09 \u306b\u5bfe\u3057\u3066\u3001\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\)\uff08\u5de6\uff09\u3001MAP \u63a8\u5b9a\uff08\u4e2d\u592e\uff09\u3001\u304a\u3088\u3073\u3001\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u3068 \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\)\uff08\u53f3\uff09\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u306e\u7d50\u679c\u3001\\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u304f\u306a\u308b\u306b\u3064\u308c\u3066 \\(e\\) \u306e MAP \u63a8\u5b9a\u3001\u3064\u307e\u308a \\(\\arg\\max _e p(e|\\tilde{x})\\) \u304c\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(\\tilde{x})\\) \u306b\u8fd1\u3065\u3044\u3066\u3044\u304f\u69d8\u5b50\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff08\u4e2d\u592e\uff09\u3002 \u307e\u305f\u3001\\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u3001\u4e8b\u5f8c\u5206\u5e03 \\(p(e|\\tilde{x})\\) \u304c \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) \u306b\u8fd1\u3065\u3044\u3066\u3044\u304f\u69d8\u5b50\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff08\u53f3\uff09\u3002</p> <pre><code># set the true distribution p(x) = f(x)\np_x = f\n# define pdf of e: p(e) = N(e|0, sigma^{-2})\np_e = sympy.exp(-0.5 * e**2 * sigma**2) * sigma / sympy.sqrt(2 * sympy.pi)\n# calculate the posterior distribution: p(e|\\tilde{x}) \\propto p(e) p(\\tilde{x}|e)\np_e_posterior = p_e * p_x.subs(x, tildex - sigma*sigma*e)\n# calculate the negative score function\nlogf = sympy.log(f)\ndlogf = sympy.diff(logf, x)  # score function\ndU = -dlogf  # negative score function. Note: U is the energy function\n\n# plot the numerical verification\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\nfor axs_, sigma_val in zip(axs, [0.1, 0.5, 1]):\n    plot_numerical_verification(\n        p_x,\n        p_e_posterior,\n        dU,\n        sigma_val = sigma_val,\n        tildex_vals = np.linspace(-5, 5, 128),\n        e_vals = np.linspace(-5, 5, 256),\n        axs=axs_,\n        fig=fig,\n        fewer_e_vals = [-3, -1, 0, 1, 3],\n    )\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/3909665571.py:22: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals, levels=100, label='p(e|tildex) without Z')\n/tmp/ipykernel_2728231/3909665571.py:32: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals/p_e_posterior_vals.max(axis=0), levels=100, label='p(e|\\\\tilde{x})/max_e p(e|\\\\tilde{x})')\n</code></pre> <p></p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#case-2-a-mixture-of-two-normal-distributions","title":"Case 2: A mixture of two normal distributions","text":"enja <p>Second, let's consider the case where \\(p(x)\\) is the mixture of two normal distributions, i.e., \\(p(x) = 0.5\\mathcal{N}(x|0, 1) + 0.5\\mathcal{N}(x|5, 1)\\) to see whether the above results hold in a more complex distribution.</p> <p>\u6b21\u306b\u3001\\(p(x)\\) \u304c2\u3064\u306e\u6b63\u898f\u5206\u5e03\u306e\u6df7\u5408\u3001\u3059\u306a\u308f\u3061 \\(p(x) = 0.5\\mathcal{N}(x|0, 1) + 0.5\\mathcal{N}(x|5, 1)\\) \u306e\u5834\u5408\u3092\u8003\u3048\u3001\u3088\u308a\u8907\u96d1\u306a\u5206\u5e03\u3067\u3082\u4e0a\u8a18\u306e\u7d50\u679c\u304c\u6210\u308a\u7acb\u3064\u304b\u3069\u3046\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p> <pre><code># define pdf of x\n# 2 modal gaussian distribution\nf = 0.5 * sympy.exp(-0.5*(x-2)**2) / sympy.sqrt(2 * sympy.pi) + 0.5 * sympy.exp(-0.5*(x+2)**2) / sympy.sqrt(2 * sympy.pi)\nf\n</code></pre> <p>\\(\\displaystyle \\frac{0.25 \\sqrt{2} e^{- 0.5 \\left(x + 2\\right)^{2}}}{\\sqrt{\\pi}} + \\frac{0.25 \\sqrt{2} e^{- 0.5 \\left(x - 2\\right)^{2}}}{\\sqrt{\\pi}}\\)</p> <pre><code>p_x = f\n# define pdf of e: p(e) = N(e|0, sigma^{-2})\np_e = sympy.exp(-0.5 * e**2 * sigma**2) * sigma / sympy.sqrt(2 * sympy.pi)\n\n# calculate the posterior distribution: p(e|\\tilde{x}) \\propto p(e) p(\\tilde{x}|e)\np_e_posterior = p_e * p_x.subs(x, tildex - sigma*sigma*e)\n\n# calculate the negative score function\nlogf = sympy.log(f)\ndlogf = sympy.diff(logf, x)  # score function\ndU = -dlogf  # negative score function. Note: U is the energy function\n\n# plot the numerical verification\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\nfor axs_, sigma_val in zip(axs, [0.1, 0.5, 1]):\n    plot_numerical_verification(\n        p_x,\n        p_e_posterior,\n        dU,\n        sigma_val = sigma_val,\n        tildex_vals = np.linspace(-5, 5, 128),\n        e_vals = np.linspace(-5, 5, 256),\n        axs=axs_,\n        fig=fig,\n        fewer_e_vals = [-3, -1, 0, 1, 3],\n    )\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/3909665571.py:22: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals, levels=100, label='p(e|tildex) without Z')\n/tmp/ipykernel_2728231/3909665571.py:32: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals/p_e_posterior_vals.max(axis=0), levels=100, label='p(e|\\\\tilde{x})/max_e p(e|\\\\tilde{x})')\n</code></pre> <p></p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#case-3-the-standard-cauchy-distribution","title":"Case 3: The standard Cauchy distribution","text":"enja <p>Third, let's consider the case where \\(p(x)\\) is the Cauchy distribution, i.e., \\(p(x) = \\frac{1}{\\pi(1+x^2)}\\) to see whether the above results hold in a distribution with heavy tails.</p> <p>3\u3064\u76ee\u306e\u4f8b\u3068\u3057\u3066\u3001\\(p(x)\\) \u304c\u30b3\u30fc\u30b7\u30fc\u5206\u5e03\u3001\u3059\u306a\u308f\u3061 \\(p(x) = \\frac{1}{\\pi(1+x^2)}\\) \u306e\u5834\u5408\u3092\u8003\u3048\u3001\u88fe\u306e\u91cd\u3044\u5206\u5e03\u3067\u3082\u4e0a\u8a18\u306e\u7d50\u679c\u304c\u6210\u308a\u7acb\u3064\u304b\u3069\u3046\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p> <pre><code># define pdf of x\n# cauchy distribution\nf = 1 / (1 + x**2) / np.pi\nf\n</code></pre> <p>\\(\\displaystyle \\frac{0.318309886183791}{x^{2} + 1}\\)</p> <pre><code>p_x = f\n# define pdf of e: p(e) = N(e|0, sigma^{-2})\np_e = sympy.exp(-0.5 * e**2 * sigma**2) * sigma / sympy.sqrt(2 * sympy.pi)\n\n# calculate the posterior distribution: p(e|\\tilde{x}) \\propto p(e) p(\\tilde{x}|e)\np_e_posterior = p_e * p_x.subs(x, tildex - sigma*sigma*e)\n\n# calculate the negative score function\nlogf = sympy.log(f)\ndlogf = sympy.diff(logf, x)  # score function\ndU = -dlogf  # negative score function. Note: U is the energy function\n\n# plot the numerical verification\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\nfor axs_, sigma_val in zip(axs, [0.1, 0.5, 1]):\n    plot_numerical_verification(\n        p_x,\n        p_e_posterior,\n        dU,\n        sigma_val = sigma_val,\n        tildex_vals = np.linspace(-5, 5, 128),\n        e_vals = np.linspace(-5, 5, 256),\n        axs=axs_,\n        fig=fig,\n        fewer_e_vals = [-3, -1, 0, 1, 3],\n    )\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/3909665571.py:22: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals, levels=100, label='p(e|tildex) without Z')\n/tmp/ipykernel_2728231/3909665571.py:32: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals/p_e_posterior_vals.max(axis=0), levels=100, label='p(e|\\\\tilde{x})/max_e p(e|\\\\tilde{x})')\n</code></pre> <p></p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#case-4-a-mixture-of-various-distributions","title":"Case 4: A mixture of various distributions","text":"enja <p>Finally, let's consider a more complex case where \\(p(x)\\) is the mixture of various distributions, a Cauchy distribution, a normal distribution and a hyperbolic secant distribution, i.e., \\(p(x) = 0.3\\frac{1}{\\pi(1+x^2)} + 0.3\\mathcal{N}(x|0, 1) + 0.4\\frac{sech(0.5\\pi x)}{2}\\).</p> <p>\u6700\u5f8c\u306b\u3001\\(p(x)\\) \u304c\u69d8\u3005\u306a\u5206\u5e03\u306e\u6df7\u5408\u3001\u30b3\u30fc\u30b7\u30fc\u5206\u5e03\u3001\u6b63\u898f\u5206\u5e03\u3001\u53cc\u66f2\u7dda\u95a2\u6570\u306e\u9006\u4f59\u63a5\u95a2\u6570\u5206\u5e03\u3001\u3059\u306a\u308f\u3061 \\(p(x) = 0.3\\frac{1}{\\pi(1+x^2)} + 0.3\\mathcal{N}(x|0, 1) + 0.4\\frac{sech(0.5\\pi x)}{2}\\) \u306e\u5834\u5408\u3092\u8003\u3048\u3001\u3088\u308a\u8907\u96d1\u306a\u5206\u5e03\u3067\u3082\u4e0a\u8a18\u306e\u7d50\u679c\u304c\u6210\u308a\u7acb\u3064\u304b\u3069\u3046\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p> <pre><code># mixed of cauchy distribution, normal distribution and hyperbolic secant distribution\nf = 0.5 * 1 / (1 + (x-3)**2) / np.pi + 0.25 * sympy.exp(-0.5*(x-2)**2) / sympy.sqrt(2 * sympy.pi) + 0.25 * sympy.sech(0.5*x*sympy.pi) / 2\nf\n</code></pre> <p>\\(\\displaystyle 0.125 \\operatorname{sech}{\\left(0.5 \\pi x \\right)} + \\frac{0.125 \\sqrt{2} e^{- 0.5 \\left(x - 2\\right)^{2}}}{\\sqrt{\\pi}} + \\frac{0.159154943091895}{\\left(x - 3\\right)^{2} + 1}\\)</p> <pre><code># define pdf of x\np_x = f\n# define pdf of e: p(e) = N(e|0, sigma^{-2})\np_e = sympy.exp(-0.5 * e**2 * sigma**2) * sigma / sympy.sqrt(2 * sympy.pi)\n\n# calculate the posterior distribution: p(e|\\tilde{x}) \\propto p(e) p(\\tilde{x}|e)\np_e_posterior = p_e * p_x.subs(x, tildex - sigma*sigma*e)\n\n# calculate the negative score function\nlogf = sympy.log(f)\ndlogf = sympy.diff(logf, x)  # score function\ndU = -dlogf  # negative score function. Note: U is the energy function\n\n# plot the numerical verification\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\nfor axs_, sigma_val in zip(axs, [0.1, 0.5, 1]):\n    plot_numerical_verification(\n        p_x,\n        p_e_posterior,\n        dU,\n        sigma_val = sigma_val,\n        tildex_vals = np.linspace(-5, 5, 128),\n        e_vals = np.linspace(-5, 5, 256),\n        axs=axs_,\n        fig=fig,\n        fewer_e_vals = [-3, -1, 0, 1, 3],\n    )\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/3909665571.py:22: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals, levels=100, label='p(e|tildex) without Z')\n/tmp/ipykernel_2728231/3909665571.py:32: UserWarning: The following kwargs were not used by contour: 'label'\n  ax.contourf(tildex_mesh, e_mesh, p_e_posterior_vals/p_e_posterior_vals.max(axis=0), levels=100, label='p(e|\\\\tilde{x})/max_e p(e|\\\\tilde{x})')\n</code></pre> <p></p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#numerical-verification-using-generated-data","title":"Numerical verification using generated data","text":"enja <p>In the previous section, we used analytically calculated posterior distributions to confirm that MAP estimation of \\(e\\) when \\(\\sigma\\) is sufficiently small approximates the score function, and that the posterior distribution can be approximated by a Gaussian distribution with the score function as the mean and \\(\\sigma^{-2}\\) as the variance.</p> <p>In this section, we will use generated data to numerically verify that estimating noise in denoising score matching becomes an estimation of the score function.</p> <p>For simplicity, we will avoid complex distributions such as Case 4 in the previous section and verify using the mixture of normal distributions \\(p(x) = 0.5\\mathcal{N}(x|-2, 1) + 0.5\\mathcal{N}(x|2, 1)\\).</p> <p>\u524d\u7bc0\u3067\u306f\u89e3\u6790\u7684\u306b\u8a08\u7b97\u3057\u305f\u4e8b\u5f8c\u5206\u5e03\u3092\u7528\u3044\u3066\u3001\\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306e \\(e\\) \u306e MAP \u63a8\u5b9a\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u8fd1\u4f3c\u3059\u308b\u3053\u3068\u3001\u305d\u3057\u3066\u4e8b\u5f8c\u5206\u5e03\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u3092\u5e73\u5747\u3001\\(\\sigma^{-2}\\) \u3092\u5206\u6563\u3068\u3059\u308b\u30ac\u30a6\u30b9\u5206\u5e03\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> <p>\u672c\u7bc0\u3067\u306f\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u3092\u6570\u5024\u7684\u306b\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <p>\u7c21\u5358\u306e\u305f\u3081\u3001\u524d\u7bc0\u306e Case 4 \u306e\u3088\u3046\u306a\u8907\u96d1\u306a\u5206\u5e03\u306f\u907f\u3051\u3066\u3001\u6df7\u5408\u6b63\u898f\u5206\u5e03 \\(p(x) = 0.5\\mathcal{N}(x|-2, 1) + 0.5\\mathcal{N}(x|2, 1)\\) \u3092\u7528\u3044\u3066\u691c\u8a3c\u3057\u307e\u3059\u3002</p> <pre><code>x = sympy.symbols('x')\nf = 0.5 * sympy.exp(-0.5*(x-2)**2) / sympy.sqrt(2 * sympy.pi) + 0.5 * sympy.exp(-0.5*(x+2)**2) / sympy.sqrt(2 * sympy.pi)\nlogf = sympy.log(f)\ndlogf = sympy.diff(logf, x)  # score function\nf\n</code></pre> <p>\\(\\displaystyle \\frac{0.25 \\sqrt{2} e^{- 0.5 \\left(x + 2\\right)^{2}}}{\\sqrt{\\pi}} + \\frac{0.25 \\sqrt{2} e^{- 0.5 \\left(x - 2\\right)^{2}}}{\\sqrt{\\pi}}\\)</p> <pre><code># generate samples\nxs = np.random.randn(10000) + 2 * ((np.random.rand(10000) &lt; 0.5) * 2 - 1)\nsigma = 0.1\nes = np.random.randn(10000) / sigma\ntildexs = xs + sigma * sigma * es\n</code></pre> enja <p>The following results are obtained using the generated \\(\\{(x_n, e_n, \\tilde{x}_n)\\}_{n=1}^{10000}\\):</p> <ul> <li>Scatter plot of \\(\\{(\\tilde{x}_n, e_n)\\}_{n=1}^{10000}\\) (blue points)</li> <li>Average value of \\(e\\) in each bin by binning \\(\\tilde{x}\\) (blue line)</li> <li>Plot of the true score function \\(s(\\tilde{x})\\) (red line)</li> </ul> <p>From this result, it can be numerically confirmed that estimating noise in denoising score matching becomes an estimation of the score function.</p> <p>Note that it is equivalent to confirming that denoising score matching works, although it is a very rough confirmation.</p> <p>\u4e0b\u8a18\u306e\u7d50\u679c\u306f\u751f\u6210\u3057\u305f \\(\\{(x_n, e_n, \\tilde{x}_n)\\}_{n=1}^{10000}\\) \u3092\u7528\u3044\u3066\u3001</p> <ul> <li>\\(\\{(\\tilde{x}_n, e_n)\\}_{n=1}^{10000}\\) \u306e\u6563\u5e03\u56f3\uff08\u9752\u70b9\uff09</li> <li>\\(\\tilde{x}\\) \u3092\u30d3\u30cb\u30f3\u30b0\u3057\u3066\u3001\u5404\u30d3\u30f3\u306b\u304a\u3051\u308b \\(e\\) \u306e\u5e73\u5747\u5024\uff08\u9752\u7dda\uff09</li> <li>\u771f\u306e\u30b9\u30b3\u30a2\u95a2\u6570 \\(s(\\tilde{x})\\) \u3092\u30d7\u30ed\u30c3\u30c8\uff08\u8d64\u7dda\uff09</li> </ul> <p>\u3092\u4e00\u3064\u306e\u30b0\u30e9\u30d5\u306b\u63cf\u753b\u3057\u305f\u3082\u306e\u3067\u3059\u3002</p> <p>\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u3092\u6570\u5024\u7684\u306b\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u304b\u306a\u308a\u7c97\u3044\u78ba\u8a8d\u3067\u306f\u3042\u308a\u307e\u3059\u304c\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u304c\u30ef\u30fc\u30af\u3059\u308b\u3053\u3068\u306e\u78ba\u8a8d\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002</p> <pre><code>df = pd.DataFrame({\n    'x': xs,\n    'e': es,\n    'tildex': tildexs,\n})\ndf['tildex_bin'] = pd.cut(df['tildex'], bins=np.linspace(-5, 5, 41))\ndfg = df.groupby('tildex_bin').e.mean().reset_index().assign(tildex_bin=lambda x: x['tildex_bin'].apply(lambda y: y.mid))\nplt.scatter(df['tildex'], df['e'], c='b', alpha=0.1, label='samples')\nplt.plot(dfg['tildex_bin'], dfg['e'], 'bx-', label='mean of e by bin of tildex')\nplt.plot(np.linspace(-5, 5, 100), -sympy.lambdify(x, dlogf)(np.linspace(-5, 5, 100)), 'r', label='score function')\nplt.ylim(-5, 5)\nplt.xlabel('tildex')\nplt.ylabel('e')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/854101565.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  dfg = df.groupby('tildex_bin').e.mean().reset_index().assign(tildex_bin=lambda x: x['tildex_bin'].apply(lambda y: y.mid))\n</code></pre> <p></p> enja <p>The following results show the histogram of \\(e\\) of the data contained in the bins of \\(\\tilde{x}\\) with a certain number of observations (density: left, cumulative density: right) plotted in blue bins. These correspond to the density function and cumulative density function of \\(p(e|\\tilde{x})\\). In particular, \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) is plotted in red as a reference.</p> <p>From the following results, it can be confirmed that \\(p(e|\\tilde{x})\\) can be approximated by \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\).</p> <p>Incidentally, it can also be confirmed that the difference between the cumulative density functions of \\(p(e|\\tilde{x})\\) and \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) is not significant by the Kolmogorov-Smirnov test.</p> <p>\u4ee5\u4e0b\u3001\u89b3\u6e2c\u6570\u304c\u3042\u308b\u7a0b\u5ea6\u591a\u3044 \\(\\tilde{x}\\) \u306e\u30d3\u30f3\u306b\u542b\u307e\u308c\u308b\u30c7\u30fc\u30bf\u306e \\(e\\) \u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\uff08\u5bc6\u5ea6\uff1a\u5de6\u3001\u7d2f\u7a4d\u5bc6\u5ea6\uff1a\u53f3\uff09\u3092\u9752\u3044\u30d3\u30f3\u3067\u30d7\u30ed\u30c3\u30c8\u3057\u305f\u7d50\u679c\u3067\u3042\u308b\u3002 \u3053\u308c\u3089\u306f \\(p(e|\\tilde{x})\\) \u306b\u76f8\u5f53\u3059\u308b\u5bc6\u5ea6\u95a2\u6570\u304a\u3088\u3073\u7d2f\u7a4d\u5bc6\u5ea6\u95a2\u6570\u3067\u3042\u308b\u3002 \u7279\u306b\u6bd4\u8f03\u5bfe\u8c61\u3068\u3057\u3066 \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) \u3092\u8d64\u3044\u7dda\u3067\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u308b\u3002</p> <p>\u4e0b\u8a18\u306e\u7d50\u679c\u3001\\(p(e|\\tilde{x})\\) \u304c \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) \u306b\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3002</p> <p>\u3064\u3044\u3067\u3067\u306f\u3042\u308b\u304c Kolmogorov-Smirnov \u691c\u5b9a\u306b\u3088\u308a\u3001\\(p(e|\\tilde{x})\\) \u3068 \\(\\mathcal{N}(e|-s(\\tilde{x}), \\sigma^{-2})\\) \u306e\u7d2f\u7a4d\u5bc6\u5ea6\u95a2\u6570\u306e\u5dee\u304c\u6709\u610f\u3067\u3042\u308b\u3068\u306f\u8a00\u3048\u306a\u3044\u3053\u3068\u3082\u78ba\u8a8d\u3067\u304d\u308b\u3002</p> <p>Warning</p> enja <p>Here, we are not trying to perform a strict statistical hypothesis test, but are simply checking the test statistics and p-values for reference. Therefore, it should be noted that it is nonsense to point out that the way of hypothesis testing is wrong or the interpretation of p-values is wrong.</p> <p>\u3053\u3053\u3067\u306f\u53b3\u5bc6\u306a\u7d71\u8a08\u7684\u4eee\u8aac\u691c\u5b9a\u3092\u884c\u3044\u305f\u3044\u8a33\u3067\u306f\u306a\u304f\u3001\u53c2\u8003\u7a0b\u5ea6\u306b\u691c\u5b9a\u7d71\u8a08\u91cf\u304a\u3088\u3073 p \u5024\u3092\u78ba\u8a8d\u3057\u3066\u3044\u308b\u7a0b\u5ea6\u3067\u3042\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u4eee\u8aac\u691c\u5b9a\u306e\u3084\u308a\u65b9\u304c\u9593\u9055\u3063\u3066\u3044\u308b\u3084 p \u5024\u306e\u89e3\u91c8\u304c\u9593\u9055\u3063\u3066\u3044\u308b\u306a\u3069\u306e\u3054\u6307\u6458\u306f\u30ca\u30f3\u30bb\u30f3\u30b9\u3067\u3042\u308b\u3053\u3068\u306b\u306f\u7559\u610f\u3055\u308c\u305f\u3044\u3002</p> <pre><code>targets = df['tildex_bin'].drop_duplicates().sort_values().pipe(lambda s: s.iloc[[int(len(s)*0.4), int(len(s)*0.75)]]).to_list()\n\nfor bin, grp in df.groupby('tildex_bin'):\n\n    if bin not in targets:\n        continue\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    fig.suptitle(f'tildex in {bin}(# of samples={len(grp)})')\n\n    ax = axs[0]\n    ax.set_title(f'histogram of e conditioned on tildex in {bin}')\n    axd = ax.twinx()\n    ax.hist(grp['e'], alpha=0.5, label=f'e conditioned on tildex in {bin}')\n    ax.axvline(x=grp['e'].mean(), color='k', label=f'mean={grp[\"e\"].mean()}')\n    axd.plot(\n        np.linspace(*grp['e'].agg(['min', 'max']), 100),\n        norm.pdf(np.linspace(*grp['e'].agg(['min', 'max']), 100), loc=-sympy.lambdify(x, dlogf)(bin.mid), scale=1/sigma),\n        'r', label=f'N(e|{-sympy.lambdify(x, dlogf)(bin.mid)}, {sigma**-2})'\n    )\n    axd.set_ylim(0)\n    ax.legend(loc='upper left')\n    axd.legend(loc='lower right')\n    ax.set_xlabel('e')\n    ax.set_ylabel('frequency')\n    axd.set_ylabel('N(e|score(tildex), sigma^{-2})')\n\n    ax = axs[1]\n    ax.hist(grp['e'], alpha=0.5, label=f'e conditioned on tildex in {bin}', cumulative=True, density=True, bins=100)\n    cdf = np.cumsum(norm.pdf(np.linspace(*grp['e'].agg(['min', 'max']), 100), loc=-sympy.lambdify(x, dlogf)(bin.mid), scale=1/sigma))\n    ax.plot(np.linspace(*grp['e'].agg(['min', 'max']), 100), (cdf-cdf.min())/(cdf.max()-cdf.min()), 'r', label=f'N(e|{-sympy.lambdify(x, dlogf)(bin.mid)}, {sigma**-2})')\n    ax.set_xlabel('e')\n    ax.set_ylabel('cumulative density')\n    ax.legend(loc='upper left')\n    ksresult = kstest(grp[\"e\"], cdf=\"norm\", args=(-sympy.lambdify(x, dlogf)(bin.mid), 1/sigma))\n    ax.set_title(f'cumulative histogram of x conditioned on tildex in {bin}:\\n kstest.statistics={ksresult.statistic:.3f}, kstest.pvalue={ksresult.pvalue:.3f}')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <pre><code>/tmp/ipykernel_2728231/1909367853.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  for bin, grp in df.groupby('tildex_bin'):\n</code></pre> <p></p> <p></p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#summary","title":"Summary","text":"enja <p>In this article, we attempted to intuitively understand that estimating noise in denoising score matching becomes an estimation of the score function using Bayesian estimation.</p> <p>As a result, using a simple one-dimensional model as an example, we showed that the score function is estimated by the MAP estimation (Maximum A Posteriori Estimation) of the scaled noise. Furthermore, we also showed that when \\(\\sigma\\) is sufficiently small, the posterior distribution of the scaled noise becomes a Gaussian distribution with the negative of the score function as the mean and \\(\\sigma^{-2}\\) as the variance, confirming that it does not contradict the empirical results of existing studies.</p> <p>Moreover, using generated data, we numerically confirmed that estimating noise in denoising score matching becomes an estimation of the score function.</p> <p>The following topics are likely to be considered as the next topics:</p> <ul> <li>interpretation of noise estimation using models of two or more dimensions;</li> <li>exploration of the objective function through the approximation of the posterior distribution of noise when the noise distribution is not normal.</li> </ul> <p>If you have any questions or suggestions for improvement regarding this article, please let us know through social media.</p> <p>From the above results, it was found that estimating noise in denoising score matching becomes an estimation of the score function. I hope this article will be helpful for those studying diffusion models.</p> <p>Thank you for reading \ud83d\udc4b</p> <p>\u672c\u8a18\u4e8b\u3067\u306f\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u3092\u30d9\u30a4\u30ba\u63a8\u5b9a\u3092\u7528\u3044\u3066\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u3053\u3068\u3092\u8a66\u307f\u307e\u3057\u305f\u3002</p> <p>\u305d\u306e\u7d50\u679c\u30011\u6b21\u5143\u306e\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3092\u984c\u6750\u3068\u3057\u3066\u3001\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30ce\u30a4\u30ba\u306e MAP \u63a8\u5b9a\uff08Maximum A Posteriori Estimation\uff09\u306b\u3088\u308a\u3001\u30b9\u30b3\u30a2\u95a2\u6570\u304c\u63a8\u5b9a\u3055\u308c\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3057\u305f\u3002 \u3055\u3089\u306b \\(\\sigma\\) \u304c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5834\u5408\u306f\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30ce\u30a4\u30ba\u306e\u4e8b\u5f8c\u5206\u5e03\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u8ca0\u306e\u5024\u3092\u5e73\u5747\u5024\u3001\\(\\sigma^{-2}\\)\u3068\u3059\u308b\u30ac\u30a6\u30b9\u5206\u5e03\u306b\u306a\u308b\u3053\u3068\u3082\u793a\u3057\u3001\u65e2\u5b58\u306e\u7814\u7a76\u306e\u7d4c\u9a13\u7684\u306a\u7d50\u679c\u3068\u77db\u76fe\u3057\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> <p>\u307e\u305f\u3001\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u3092\u6570\u5024\u7684\u306b\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> <p>\u6b21\u306e\u8a71\u984c\u3068\u3057\u3066\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u6319\u3052\u3089\u308c\u307e\u3059\uff1a</p> <ul> <li>\u4e8c\u6b21\u5143\u4ee5\u4e0a\u306e\u30e2\u30c7\u30eb\u3092\u984c\u6750\u3068\u3057\u305f\u30ce\u30a4\u30ba\u306e\u63a8\u5b9a\u306e\u89e3\u91c8</li> <li>\u30ce\u30a4\u30ba\u306e\u5206\u5e03\u304c\u6b63\u898f\u5206\u5e03\u3067\u306f\u306a\u3044\u5834\u5408\u306e\u30ce\u30a4\u30ba\u306e\u4e8b\u5f8c\u5206\u5e03\u306e\u8fd1\u4f3c\u3092\u901a\u3058\u305f\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306e\u76ee\u7684\u95a2\u6570\u306e\u63a2\u7d22</li> </ul> <p>\u672c\u8a18\u4e8b\u306b\u95a2\u3059\u308b\u8cea\u554f\u3084\u6539\u5584\u63d0\u6848\u304c\u3042\u308c\u3070\u3001\u30bd\u30fc\u30b7\u30e3\u30eb\u30e1\u30c7\u30a3\u30a2\u7b49\u3092\u901a\u3058\u3066\u304a\u77e5\u3089\u305b\u304f\u3060\u3055\u3044\u3002</p> <p>\u4ee5\u4e0a\u306e\u7d50\u679c\u306b\u3088\u308a\u3001\u30c7\u30ce\u30a4\u30b8\u30f3\u30b0\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u304a\u3044\u3066\u30ce\u30a4\u30ba\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u30b9\u30b3\u30a2\u95a2\u6570\u306e\u63a8\u5b9a\u306b\u306a\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002 \u3053\u306e\u8a18\u4e8b\u304c\u62e1\u6563\u30e2\u30c7\u30eb\u3092\u52c9\u5f37\u3057\u3066\u3044\u308b\u4eba\u306e\u5f79\u306b\u7acb\u3066\u308c\u3070\u5b09\u3057\u3044\u3067\u3059</p> <p>\u305d\u308c\u3067\u306f\u3001\u6700\u5f8c\u307e\u3067\u8aad\u3093\u3067\u3044\u305f\u3060\u304d\u3001\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f \ud83d\udc4b</p>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/2024/11/23/denoising-score-matching-bayesian-interpretation-of-why-the-noise-prediction-model-denoising-model-can-estimate-the-score-function/#references","title":"References","text":"<ol> <li> <p>P. Vincent, \"A Connection Between Score Matching and Denoising Autoencoders,\" Neural Computation, 2011. \u21a9\u21a9</p> </li> <li> <p>P. Vincent, \"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,\" Journal of Machine Learning Research, 2010. \u21a9\u21a9</p> </li> <li> <p>\u5ca1\u91ce\u539f\u5927\u8f14, \"\u62e1\u6563\u3068\u6d41\u308c\u306b\u57fa\u3065\u304f\u5b66\u7fd2\u3068\u63a8\u8ad6\", \u795e\u7d4c\u56de\u8def\u5b66\u4f1a2023 \u5168\u56fd\u5927\u4f1a \u57fa\u8abf\u8b1b\u6f14, 2023. \u21a9\u21a9</p> </li> <li> <p>Y. Song and S. Ermon, \"Generative Modeling by Estimating Gradients of Data Distribution\", In Proceedings of NeurIPS, 2019. \u21a9</p> </li> </ol>","tags":["denoising score matching","diffusion models","generative models"]},{"location":"articles/archive/2024/","title":"2024","text":""},{"location":"articles/category/data-science/","title":"Data Science","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#denoising-score-matching","title":"denoising score matching","text":"<ul> <li>[Denoising Score Matching] Bayesian Interpretation of Why the Noise Prediction Model (Denoising Model) Can Estimate the Score Function</li> </ul>"},{"location":"tags/#diffusion-models","title":"diffusion models","text":"<ul> <li>[Denoising Score Matching] Bayesian Interpretation of Why the Noise Prediction Model (Denoising Model) Can Estimate the Score Function</li> </ul>"},{"location":"tags/#generative-models","title":"generative models","text":"<ul> <li>[Denoising Score Matching] Bayesian Interpretation of Why the Noise Prediction Model (Denoising Model) Can Estimate the Score Function</li> </ul>"}]}